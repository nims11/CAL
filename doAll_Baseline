#!/bin/bash
#Assume the index has been built
# rm -rf lemurindex
# mkdir lemurindex
# mkdir corpdata
# cd corpdata ; tar xf ../$ZIPF ; cd ../ 
# python clustering/generateLemurIndexXML.py --index=diskindex --raw=~/TREC/Corpus/robust04  --xmloutput=index/disk.xml
# ~/Develop/lemur/bin/IndriBuildIndex index/disk.xml
# rm -rf corpdata


export LC_ALL=C
export LANG=C

PURPOSE=baseline
JUDGECLASS=${CORP:-oldreut}
CORPLIST=($JUDGECLASS)
SOFIA=${SOFIA:-$(dirname $(readlink -f $0))/sofia-ml/src/sofia-ml}
MAXTHREADS=${MAXTHREADS:-4}
KISSSDB=$(dirname $(readlink -f $0))/kisssdb/kisssdb

if [ ! -e "$KISSSDB" ]; then
    echo "kisssdb binary not found! 'make' at the root of this project"
    exit 1
fi

if [ ! -e "$SOFIA" ]; then
    echo "sofia binary not found at $SOFIA!"
    exit 1
fi

trap "exit 1" TERM
export _PPID=$$

function process_corpus(){
    pushd Corpus > /dev/null
    CORP=$1
    echo "Processing Corpus $CORP"
    if [[ "$MODE" == "4gram" ]]; then
        TARPROG=./tar4
    else
        TARPROG=./tar
    fi

    zcat $CORP.tgz \
        | "$TARPROG" /dev/stdin \
        2> >(awk '/Found file/{x+=1;printf("%d files read\r", x);} END{print "\ndone"}' > /dev/stderr) \
        | sort -k3 > "$CORP.concordance" 2> dofast.stderr

    mv df "$CORP.df"
    cat -n "$CORP.df" | join -13 -23 - "$CORP.concordance" | sort -k4,4 -k2,2n > "$CORP.tfdf"

    tar -tzf "$CORP".tgz | grep -v '/$' | wc -l > "$CORP".N
    N=`cat "$CORP.N"`
    echo "Number of documents: $N"
    ../dotfidfinline $N < "$CORP.tfdf" > "$CORP.svm.fil"
    sort -k1,1 "$CORP".svm.fil > "$CORP".svm.fil.sorted; mv "$CORP".svm.fil.sorted "$CORP".svm.fil

    echo "Indexing $CORP.svm.fil"
    awk '{print $1;print $0;}' "$CORP.svm.fil" | "$KISSSDB" "$CORP".db 1000081
    if [ ${PIPESTATUS[0]} -eq 0 ]; then
        echo "DB Population complete..."
    else
        echo "Error Creating DB"
        kill -s TERM $_PPID
    fi
    popd > /dev/null
}

for CORP in "${CORPLIST[@]}"; do
    process_corpus "$CORP"

    echo "Counting dimensions..."
    DIMENSIONALITY=$(awk 'BEGIN{a=0;RS=":"}{if(a<$NF){a=$NF}}END{a+=1;print a}' Corpus/"$CORP".svm.fil)
    echo "Dimensions = $DIMENSIONALITY"

    while IFS='' read -r line || [[ -n $line ]]; do
        IFS=':' read -ra TEXT <<< "$line"

        TOPIC="${TEXT[0]}"
        QUERY="${TEXT[1]}"
        echo "$TOPIC: $QUERY"

        rm -rf result/"$PURPOSE"/"$CORP"/"$TOPIC"/
        mkdir -p result/"$PURPOSE"/"$CORP"/
        mkdir -p result/dump/"$PURPOSE"/"$CORP"/

        rm -rf $TOPIC
        mkdir $TOPIC

        pushd $TOPIC 

        echo "$QUERY" > "$TOPIC".seed.doc

        cut -d' ' -f1 ../Corpus/"$CORP.svm.fil" | sed -e 's/.*/& &/' \
        | cut -d' ' -f1 | cat -n > docfils

        touch rel.$TOPIC.fil

        touch prel.$TOPIC
        rm -rf prevalence.rate
        touch prevalence.rate
        rm -rf rel.rate
        touch rel.rate


        rm -f new[0-9][0-9].$TOPIC tail[0-9][0-9].$TOPIC self*.$TOPIC gold*.$TOPIC
        touch new00.$TOPIC


        NDOCS=`cat docfils | wc -l`
        NDUN=0
        L=1
        R=100
        export LAMBDA=0.0001

        cp $TOPIC.seed.doc ../$TOPIC.seed.doc
        popd

        ./dofeaturesseed $TOPIC.seed.doc $TOPIC $CORP
        pushd $TOPIC
        sed -e 's/[^ ]*/0/' ../Corpus/$CORP.svm.fil | ../dosplit
        sed -e 's/[^ ]*/1/' svm.$TOPIC.seed.doc.fil > $TOPIC.synthetic.seed


        for N in $(seq -f "%02g" 0 99); do
            if [ $NDUN -lt $NDOCS ] ; then
                cp $TOPIC.synthetic.seed trainset
                cut -f2 docfils | shuf -n$R | sort |\
                    "$KISSSDB" ../$CORP.db | sed -e's/[^ ]*/-1/' > trainset1 &

                (
                cat new[0-9][0-9].$TOPIC > seed
                cat seed | sort | join - rel.$TOPIC.fil | sed -e 's/^/1 /' > x
                cat seed | sort | join -v1 - rel.$TOPIC.fil | shuf -n 50000 | sed -e 's/^/-1 /' >> x
                cut -d' ' -f2 x | "$KISSSDB" ../$CORP.db | cut -d' ' -f2- |\
                    paste -d' ' <(cut -d' ' -f1 x) - | sort -n > trainset2
                ) &
                wait
                cat trainset1 trainset2 >> trainset
                rm trainset1 trainset2


                #Calculate relevant documents prevalence rate in the traning set

                RELTRAINDOC=`grep -E "^1\b" trainset | wc -l`
                NOTRELTRAINDOC=`grep -E "^-1\b" trainset | wc -l`
                PREVALENCERATE=`echo "scale=4; $RELTRAINDOC / ($RELTRAINDOC + $NOTRELTRAINDOC)" | bc`
                echo $RELTRAINDOC $NOTRELTRAINDOC $PREVALENCERATE >> prevalence.rate


                "$SOFIA" --learner_type logreg-pegasos --loop_type roc --lambda $LAMBDA\
                    --iterations 200000 --training_file trainset --dimensionality $DIMENSIONALITY --model_out svm_model

                RES=$?
                echo $RES
                if [ "$RES" -eq "0" ] ; then
                    for z in svm.test.* ; do
                        while [ "$(jobs | grep 'Running' | wc -l)" -ge "$MAXTHREADS" ]; do
                            sleep 1
                        done
                        "$SOFIA" --test_file $z --dimensionality $DIMENSIONALITY --model_in svm_model --results_file pout.$z &
                        #/home/user/svmlight/svm_classify $z svm_model pout.$z
                    done
                    wait
                else
                    rm -f pout.svm.test.*
                    cut -f2 docfils | sort -R | cat -n | sort -k2 | sed -e 's/ */-/' > pout.svm.test.1
                fi
                cut -f1 pout.svm.test.* | ../fixnum | cat -n | join -o2.2,1.2 -t$'\t' - docfils > inlr.out
                awk \
                    'NR==FNR{a[$1]=1}NR!=FNR{if(!a[$1])print $0}' \
                    seed inlr.out | sort -rn -k2 | cut -d' ' -f1 > new$N.$TOPIC
                # sort seed | join -v2 - inlr.out | sort -rn -k2 | cut -d' ' -f1 > new$N.$TOPIC
                cat new[0-9][0-9].$TOPIC > x
                if [ "$N" != "99" ] ; then
                    head -$L new$N.$TOPIC > y ; mv y new$N.$TOPIC
                fi

                python ../doJudgementMain.py --topic=$TOPIC --judgefile=../judgement/qrels.$JUDGECLASS.list\
                    --input=new$N.$TOPIC --output=rel.$TOPIC.Judged.doc.list --record=$TOPIC.record.list
                # rm -rf rel.$TOPIC.Judged.doc.list
                # touch rel.$TOPIC.Judged.doc.list
                # while IFS='' read -r line || [[ -n $line ]]; do
                #    RELFLAG=`cat ../judgement/qrels.$JUDGECLASS.list | grep "$TOPIC 0 $line 1" | wc -l`

                #    if [ $RELFLAG -gt "0" ] ; then
                #       echo $line 1 >> rel.$TOPIC.Judged.doc.list
                #       echo $line 1 >> $TOPIC.record.list
                #    else
                #       echo $line 0 >> $TOPIC.record.list
                #    fi
                # done < new$N.$TOPIC
                cat rel.$TOPIC.Judged.doc.list >> rel.$TOPIC.fil
                cat rel.$TOPIC.Judged.doc.list > rel.$TOPIC.$N.Judged.doc.list

                RELFINDDOC=`wc -l < rel.$TOPIC.Judged.doc.list`
                RELRATE=`echo "scale=4; $RELFINDDOC / $L" | bc`
                CURRENTREL=`wc -l < rel.$TOPIC.fil`
                echo $RELFINDDOC $L $RELRATE $CURRENTREL >> rel.rate

                sort rel.$TOPIC.fil | sed -e 's/$/ 1/' > prel.$TOPIC

                cut -d' ' -f1 prel.$TOPIC > rel.$TOPIC.fil

                NDUN=$((NDUN+L))
                L=$((L+(L+9)/10))
                # NUM_REL=$(cat rel.$TOPIC.fil | sort | uniq | wc -l)
                # TOT_REL=$(grep "^$TOPIC.*[1-9]$" ../judgement/qrels.$JUDGECLASS.list | cut -d' ' -f3 | sort | uniq | wc -l)
                # if [ $NUM_REL -eq $TOT_REL ]; then
                #     break
                # fi
            fi
        done

        rm -rf svm.test.*
        popd

        mv $TOPIC result/"$PURPOSE"/"$CORP"/$TOPIC
        rm $TOPIC.seed.doc

    done < "judgement/$CORP.topic.stemming.txt"
    # rm -rf "$CORP".svm.fil
    # rm "$CORP".df
    # rm "$CORP".db

    rm N

done
